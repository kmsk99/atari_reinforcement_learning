알겠습니다. 각 단계별로 더 상세하게, 코드 수정 위치와 내용을 포함하여 설명하겠습니다. 기존 `ppo_breakout.py` 및 `utils.py` 파일의 내용을 수정하는 방식으로 안내합니다.

---

**사전 준비: 필요한 라이브러리 확인**

`gymnasium[atari]` 및 `ale-py`가 제대로 설치되어 있는지 확인합니다. `autorom` 설치도 필요할 수 있습니다.

```bash
pip install "gymnasium[atari]" ale-py autorom Pillow pytz pandas matplotlib imageio torch torchvision numpy
# CUDA 사용 시 맞는 버전의 torch 설치 확인
```

---

### 0단계: 실험 추적을 위한 폴더 구조 설정 (코드 시작 부분 수정)

**목표:** 각 학습 실행 결과를 별도의 폴더에 저장하여 체계적으로 관리하고, 어떤 설정으로 실행했는지 쉽게 파악합니다.

**적용 방법:** `ppo_breakout.py` 파일 상단에 다음 코드를 추가하거나 수정합니다.

```python
# 라이브러리 임포트
import math
import random
import argparse
import os
import numpy as np
import gymnasium as gym
from gymnasium.vector import AsyncVectorEnv
import imageio
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from torch.cuda.amp import autocast, GradScaler
from torch.distributions import Categorical
# utils 임포트는 그대로 두거나, 필요시 경로 수정
from utils import (
    load_checkpoint,        # 수정 필요 (경로 처리)
    plot_all_scores,        # 수정 필요 (경로 처리)
    save_checkpoint,        # 수정 필요 (경로 처리)
    visualize_filters,      # 수정 필요 (경로 처리)
    visualize_layer_output, # 수정 필요 (경로 처리)
    save_gameplay_gif,    # 수정 필요 (경로 처리)
    get_korea_time,
    make_env,             # 수정 필요 (래퍼 변경)
    compute_gae,
)
from datetime import datetime
import pytz
import time

# --- 실험 추적 설정 추가 ---
# 기본 경로 설정 (ppo_breakout.py 파일이 있는 위치 기준)
current_script_path = os.path.dirname(os.path.realpath(__file__))
# 'exp' 대신 'results' 폴더 사용 제안, 스크립트 위치의 상위 폴더에 생성
base_results_dir = os.path.join(os.path.dirname(current_script_path), "results")

# 한국 시간 기준 폴더명 생성
korea_tz = pytz.timezone('Asia/Seoul')
now_kst = datetime.now(korea_tz)
timestamp = now_kst.strftime("%Y%m%d_%H%M%S")

# 이 실험의 주요 특징 태그 (수동 설정)
# 예: 초기 설정, 래퍼 변경, 하이퍼파라미터 변경 등
experiment_tag = "Baseline_PPO_Breakout" # <<--- 여기를 실험 내용에 맞게 수정하세요!

# 최종 결과 폴더 경로 생성
run_name = f"{timestamp}_{experiment_tag}"
output_dir = os.path.join(base_results_dir, run_name)

# 결과 폴더 및 하위 폴더 생성 (checkpoints, filters, layers, gameplay)
checkpoint_dir = os.path.join(output_dir, "checkpoints")
filter_dir = os.path.join(output_dir, "filters") # 사용 안 할 경우 주석 처리 가능
layer_dir = os.path.join(output_dir, "layers")   # 사용 안 할 경우 주석 처리 가능
gameplay_dir = os.path.join(output_dir, "gameplay")
plot_filename = os.path.join(output_dir, "training_") # plot_all_scores에 전달될 접두사 포함 경로
csv_filename = os.path.join(output_dir, "scores.csv") # plot_all_scores에 전달될 CSV 경로

os.makedirs(checkpoint_dir, exist_ok=True)
# os.makedirs(filter_dir, exist_ok=True) # 필요 없으면 주석 처리
# os.makedirs(layer_dir, exist_ok=True)   # 필요 없으면 주석 처리
os.makedirs(gameplay_dir, exist_ok=True)

print(f"--- 실험 시작: {run_name} ---")
print(f"결과 저장 경로: {output_dir}")
# --- 실험 추적 설정 끝 ---


# CUDA 사용 가능 여부 확인 (기존 코드 유지)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- 하이퍼파라미터 설정 (아래 단계들에서 수정될 예정) ---
# 학습 관련 파라미터
GAMMA = 0.99
LAMBDA = 0.95
# BATCH_SIZE = 32 # 미니배치 로직 변경으로 이 변수는 직접 사용 안 함
NUM_ENVS = 32
LEARNING_RATE = 0.001 # 2단계에서 수정
OPTIMIZER_GAMMA = 0.999 # 2단계에서 제거 또는 수정
PPO_EPOCHS = 4        # 3단계에서 수정
PPO_EPSILON = 0.2
ENTROPY_COEF = 0.01     # 4단계에서 수정 (선택적)
VALUE_COEF = 0.5
MAX_GRAD_NORM = 0.5
STEPS_PER_UPDATE = 128  # 2단계에서 수정
MAX_EPISODE_STEPS = 5000 # 또는 AtariPreprocessing의 TimeLimit과 연동되므로 더 길게 설정 가능 (예: 27000 프레임 ~ 10분)

# 환경 및 모델 파라미터 (AtariPreprocessing 사용으로 일부 자동화됨)
ENV_ID = "ALE/Breakout-v5"
FRAME_STACK = 4 # AtariPreprocessing과 FrameStack 래퍼에서 사용
# FRAME_SIZE = 84 # AtariPreprocessing에서 설정
# CONV 필터 및 FC 크기는 그대로 유지 (필요시 추후 조정)
CONV1_FILTERS = 32
CONV2_FILTERS = 64
CONV3_FILTERS = 64
FC_SIZE = 512

# 시각화 및 체크포인트 관련 파라미터 (기존 유지)
VISUALIZATION_INTERVAL = 100
GAMEPLAY_GIF_INTERVAL = 500
CHECKPOINT_INTERVAL = 500 # GIF 저장 시 체크포인트도 저장되므로 간격 조절 가능

# --- 신경망 정의 (PPONet) - 수정 없음 ---
class PPONet(nn.Module):
    def __init__(self, num_actions):
        super(PPONet, self).__init__()
        # CNN 특징 추출기 (입력 채널은 FRAME_STACK과 동일)
        self.conv1 = nn.Conv2d(FRAME_STACK, CONV1_FILTERS, kernel_size=8, stride=4) # Input: [N, 4, 84, 84]
        self.conv2 = nn.Conv2d(CONV1_FILTERS, CONV2_FILTERS, kernel_size=4, stride=2) # Output Conv1: [N, 32, 20, 20]
        self.conv3 = nn.Conv2d(CONV2_FILTERS, CONV3_FILTERS, kernel_size=3, stride=1) # Output Conv2: [N, 64, 9, 9]
                                                                                      # Output Conv3: [N, 64, 7, 7]

        # 특성 맵 크기 계산 (자동 계산 방식 권장 또는 확인 필요)
        # CNN 출력 크기 확인 (예시 입력 사용)
        with torch.no_grad():
            dummy_input = torch.zeros(1, FRAME_STACK, 84, 84)
            c1 = F.relu(self.conv1(dummy_input))
            c2 = F.relu(self.conv2(c1))
            c3 = F.relu(self.conv3(c2))
            flattened_size = c3.flatten(1).shape[1] # [1, 64*7*7] -> 3136
            print(f"Calculated CNN output size: {flattened_size}") # 3136 확인
        conv_output_size = flattened_size # 64 * 7 * 7 = 3136

        self.flatten = nn.Flatten()

        # 특징 추출기
        self.feature_extractor = nn.Sequential(
            self.conv1, nn.ReLU(),
            self.conv2, nn.ReLU(),
            self.conv3, nn.ReLU(),
            self.flatten
        )

        # Actor 네트워크 (정책)
        self.actor = nn.Sequential(
            nn.Linear(conv_output_size, FC_SIZE), nn.ReLU(),
            nn.Linear(FC_SIZE, num_actions)
        )

        # Critic 네트워크 (가치)
        self.critic = nn.Sequential(
            nn.Linear(conv_output_size, FC_SIZE), nn.ReLU(),
            nn.Linear(FC_SIZE, 1)
        )

        self.activations = {}

    def forward(self, x):
        # AtariPreprocessing(scale_obs=True) 사용 시 입력 정규화 불필요
        # if x.dtype == torch.uint8: # 혹시 uint8로 들어오면 float 변환 및 스케일링
        #     x = x.float() / 255.0
        features = self.feature_extractor(x)
        action_logits = self.actor(features)
        state_values = self.critic(features)
        return action_logits, state_values

    def get_action_and_value(self, obs, action=None):
        # 입력 타입 및 장치 처리
        if isinstance(obs, np.ndarray):
            obs = torch.from_numpy(obs).float() # 이미지는 float로 가정
        if obs.device != device:
             obs = obs.to(device)

        # 이미지가 0-1 스케일인지 확인 (AtariPreprocessing(scale_obs=True) 사용 시 가정)
        # assert obs.max() <= 1.0 and obs.min() >= 0.0, "Observations not scaled to [0, 1]"

        # 혼합 정밀도 추론
        with torch.cuda.amp.autocast(enabled=scaler.is_enabled()): # scaler 상태 따라 자동 조절
            logits, values = self.forward(obs)
            probs = F.softmax(logits, dim=-1)
            dist = Categorical(probs=probs) # probs 사용 권장

            if action is None:
                action = dist.sample()
            else: # 액션이 주어졌을 때, 해당 액션의 장치 확인
                if action.device != logits.device:
                    action = action.to(logits.device)

            log_prob = dist.log_prob(action)
            entropy = dist.entropy()

        # values 텐서 차원 정리 [batch, 1] -> [batch]
        if values.dim() > 1 and values.size(-1) == 1:
            values = values.squeeze(-1)

        return action, log_prob, entropy, values

    # register_hooks 함수는 그대로 유지
    def register_hooks(self, layer_names):
        # ... (기존 코드) ...
        pass

# --- PPO 업데이트 함수 (train_ppo) - 3단계에서 수정 예정 ---
# def train_ppo(...):
# ... 기존 코드 ...

# --- 메인 함수 정의 (main) ---
def main(render):
    # 시간대 설정 코드는 이미 위에서 처리됨

    # --- 결과 저장 경로 사용 ---
    print(f"체크포인트 저장 경로: {checkpoint_dir}")
    print(f"게임 플레이 GIF 저장 경로: {gameplay_dir}")
    # ... 기타 경로 출력 ...

    if render:
        # 렌더링 모드 시 make_env 수정 적용 필요
        env = make_env(ENV_ID, render=True, max_episode_steps=MAX_EPISODE_STEPS, frame_stack=FRAME_STACK, clip_rewards=False)() # 렌더링 시 보상 클립 안함
        num_envs = 1
    else:
        # 병렬 환경 생성 시 수정된 make_env 사용
        envs = AsyncVectorEnv(
            [make_env(ENV_ID, max_episode_steps=MAX_EPISODE_STEPS, frame_stack=FRAME_STACK, clip_rewards=True) for _ in range(NUM_ENVS)]
        )
        num_envs = NUM_ENVS

    # 액션 공간 크기 가져오기 (Atari 환경은 동일)
    # if render:
    #     num_actions = env.action_space.n
    # else:
    #     # AsyncVectorEnv는 action_space 속성 제공
    #     num_actions = envs.single_action_space.n
    sample_env = make_env(ENV_ID)() # 샘플 환경으로 액션 수 확인
    num_actions = sample_env.single_action_space.n if hasattr(sample_env, 'single_action_space') else sample_env.action_space.n
    sample_env.close()
    print(f"환경: {ENV_ID}, 액션 수: {num_actions}")

    # PPO 모델 생성 (기존 유지)
    model = PPONet(num_actions).to(device)
    # model.register_hooks(["conv1", "conv2", "conv3"]) # 필요시 주석 해제

    # Optimizer와 Scheduler 초기화 (2단계에서 수정 예정)
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=OPTIMIZER_GAMMA)
    scaler = GradScaler() # 혼합 정밀도 학습용 (활성화 여부 조절 가능)

    # --- 체크포인트 로드 (경로 수정) ---
    saved_scores = []
    # checkpoint_dir를 인자로 전달
    checkpoint_data, loaded_scores = load_checkpoint(checkpoint_dir=checkpoint_dir)
    start_episode = 0
    episode_count = 0 # 총 완료된 에피소드 수

    if checkpoint_data:
        # state_dict 로드 전에 모델을 device로 보냄
        model.load_state_dict(checkpoint_data["model_state"])
        optimizer.load_state_dict(checkpoint_data["optimizer_state"])
        if "scaler_state" in checkpoint_data: # 이전 버전 호환성
             scaler.load_state_dict(checkpoint_data["scaler_state"])
        if "scheduler_state" in checkpoint_data: # 이전 버전 호환성
            scheduler.load_state_dict(checkpoint_data["scheduler_state"])
        start_episode = checkpoint_data.get("episode", 0) # n_epi 기준
        # episode_count는 scores 길이로 복원하는 것이 더 정확할 수 있음
        if loaded_scores is not None and len(loaded_scores) > 0:
            saved_scores = loaded_scores
            episode_count = len(saved_scores) # 완료된 에피소드 수 복원
            print(f"체크포인트 로드 완료. 누적 에피소드 {episode_count} (업데이트 {start_episode})부터 이어갑니다.")
            print(f"로드된 점수 {len(saved_scores)}개, 최근 평균: {np.mean(saved_scores[-100:]):.2f}")
        else:
            print(f"체크포인트는 로드했지만 점수 데이터가 없습니다. 에피소드 {start_episode}부터 시작합니다.")
            saved_scores = []
            episode_count = 0 # 재시작 시 0으로
    else:
        print("체크포인트가 없습니다. 새로운 학습을 시작합니다.")
        saved_scores = []
        episode_count = 0

    # --- 렌더링 모드 (경로 및 함수 호출 수정) ---
    if render:
        # 렌더링 모드에서는 평가만 수행
        print("--- 렌더링 모드 시작 ---")
        # plot_all_scores 경로 수정 필요
        plot_all_scores(saved_scores, plot_filename, csv_filename) # 그래프 먼저 그리기

        for idx in range(5): # 5번 플레이
            frames = []
            # reset()은 (obs, info) 튜플 반환
            obs, _ = env.reset()
            done = False
            total_reward = 0
            step_count = 0

            while not done and step_count < MAX_EPISODE_STEPS * 2: # 최대 스텝 제한
                # obs 타입 확인 및 텐서 변환
                if isinstance(obs, tuple): # FrameStack LazyFrames 처리
                     obs = np.array(obs)
                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)

                # 액션 선택 (no_grad 필수)
                with torch.no_grad():
                    action, _, _, _ = model.get_action_and_value(obs_tensor)
                    action = action.cpu().item()

                # env.step() 반환값: obs, reward, terminated, truncated, info
                next_obs, r, terminated, truncated, info = env.step(action)
                frames.append(env.render()) # env.render() 호출

                done = terminated or truncated # 종료 조건
                total_reward += r
                obs = next_obs
                step_count += 1

            print(f"평가 에피소드 {idx+1} 완료. 점수: {total_reward}, 스텝: {step_count}")
            gif_path = os.path.join(gameplay_dir, f"Breakout_eval_{idx+1}_score_{int(total_reward)}.gif")
            imageio.mimsave(gif_path, frames, fps=30)
            print(f"GIF 저장 완료: {gif_path}")
        env.close()
        return # 렌더링 모드는 여기서 종료

    # --- 학습 모드 ---
    print("--- 학습 모드 시작 ---")
    # 초기 상태 설정 (reset 반환값 확인)
    states, _ = envs.reset() # AsyncVectorEnv는 (obs, info) 반환할 수 있음, obs만 필요

    total_steps = start_episode * STEPS_PER_UPDATE * NUM_ENVS # 대략적인 복원
    episode_rewards = np.zeros(num_envs, dtype=np.float32) # 각 환경별 현재 누적 보상
    episode_lengths = np.zeros(num_envs, dtype=np.int32)   # 각 환경별 현재 스텝 수

    # 메인 학습 루프 (100_001 -> 더 길게 설정 가능, 예: 1,000,000 업데이트)
    num_updates = 100_001
    start_time = time.time()

    for update in range(start_episode + 1, num_updates + 1): # 1부터 시작 또는 이어서
        # 데이터 수집 버퍼 초기화
        mb_obs, mb_actions, mb_logprobs, mb_values, mb_rewards, mb_dones = [], [], [], [], [], []

        # STEPS_PER_UPDATE 스텝 동안 데이터 수집
        for step in range(STEPS_PER_UPDATE):
            total_steps += num_envs

            # 상태 텐서 변환 (AsyncVectorEnv는 이미 numpy array)
            current_states_np = np.array(states) # LazyFrames 처리 확인
            current_states_tensor = torch.FloatTensor(current_states_np).to(device)

            # 모델로부터 행동, 로그 확률, 가치 얻기 (no_grad 사용)
            with torch.no_grad():
                actions_tensor, log_probs_tensor, _, values_tensor = model.get_action_and_value(current_states_tensor)

            # CPU로 이동 후 numpy 변환 (환경 스텝용)
            actions_np = actions_tensor.cpu().numpy()

            # 데이터 저장 (CPU에 저장하여 GPU 메모리 절약)
            mb_obs.append(current_states_np) # numpy 상태 저장
            mb_actions.append(actions_np)
            mb_logprobs.append(log_probs_tensor.cpu().numpy())
            mb_values.append(values_tensor.cpu().numpy())

            # 환경에서 스텝 실행 (next_states는 numpy array)
            next_states, rewards_np, terminated, truncated, infos = envs.step(actions_np)

            # 완료 여부 (bool 배열)
            dones_np = np.logical_or(terminated, truncated)

            # 보상 및 종료 상태 저장
            mb_rewards.append(rewards_np)
            mb_dones.append(dones_np)

            # 현재 상태 업데이트
            states = next_states

            # 에피소드 통계 업데이트 및 로깅
            episode_rewards += rewards_np
            episode_lengths += 1

            # 'final_info'는 벡터 환경 전체에 대한 정보 딕셔너리 배열 또는 None
            if "final_info" in infos:
                for idx, info in enumerate(infos["final_info"]):
                    # info가 None이 아닐 때 (실제 에피소드 종료 발생)
                    if info is not None and 'episode' in info:
                        real_done = True # 실제 에피소드 종료 (TimeLimit 아님)
                        ep_rew = info['episode']['r'][0] # 보상
                        ep_len = info['episode']['l'][0] # 길이

                        saved_scores.append(ep_rew) # 점수 리스트에 추가
                        episode_count += 1 # 완료된 에피소드 수 증가

                        # 터미널에 로그 출력 (너무 자주 출력되지 않도록 조절 가능)
                        if episode_count % 10 == 0: # 10 에피소드마다 출력
                            current_time_str = get_korea_time()
                            avg_score_100 = np.mean(saved_scores[-100:]) if len(saved_scores) >= 100 else np.mean(saved_scores)
                            elapsed_time = time.time() - start_time
                            steps_per_sec = total_steps / elapsed_time if elapsed_time > 0 else 0
                            print(f"[{current_time_str}] Upd {update}/{num_updates}, Epi {episode_count}, Step {total_steps}, "
                                  f"AvgScore(100): {avg_score_100:.1f}, LR: {scheduler.get_last_lr()[0]:.1e}, "
                                  f"SPS: {int(steps_per_sec)}")

                        # 그래프 그리기 (간격 조절)
                        if episode_count % VISUALIZATION_INTERVAL == 0 and episode_count > 0:
                            plot_all_scores(saved_scores, plot_filename, csv_filename) # 경로 전달
                            print(f"에피소드 {episode_count}: 점수 그래프 업데이트 완료.")
                            # 필터/레이어 시각화 (필요시 주석 해제 및 경로 수정)
                            # visualize_filters(model, "conv1", episode_count, save_path=output_dir)
                            # visualize_layer_output(model, "conv1", output_dir, episode_count)

                        # GIF 저장 및 체크포인트 저장 (간격 조절)
                        if episode_count % GAMEPLAY_GIF_INTERVAL == 0 and episode_count > 0:
                             print(f"에피소드 {episode_count}: 게임 플레이 GIF 및 체크포인트 저장 중...")
                             # save_gameplay_gif 함수에 gameplay_dir 경로 전달
                             reward = save_gameplay_gif(model, ENV_ID, episode_count, max_steps=MAX_EPISODE_STEPS * 2, device=device, save_path=gameplay_dir)
                             print(f"게임 플레이 GIF 저장 완료 (점수: {reward}).")

                             # save_checkpoint 함수에 checkpoint_dir 경로 전달
                             save_checkpoint(
                                 {
                                     "model_state": model.state_dict(),
                                     "optimizer_state": optimizer.state_dict(),
                                     "scaler_state": scaler.state_dict(),
                                     "scheduler_state": scheduler.state_dict(),
                                     "episode": update, # 현재 업데이트 번호 저장
                                 },
                                 episode_count, # 에피소드 번호 기준 저장/삭제
                                 saved_scores,
                                 checkpoint_dir=checkpoint_dir # 경로 전달
                             )
                             print(f"체크포인트 저장 완료: episode_{episode_count}.pth")

                        # 환경 리셋은 AsyncVectorEnv가 자동으로 처리 (dones_np가 True일 때)
                        if real_done:
                             episode_rewards[idx] = 0.0
                             episode_lengths[idx] = 0


        # --- GAE 계산 및 PPO 업데이트 ---
        # 마지막 상태의 가치 계산 (부트스트래핑)
        with torch.no_grad():
            final_states_tensor = torch.FloatTensor(np.array(states)).to(device)
            _, _, _, next_value_tensor = model.get_action_and_value(final_states_tensor)
            next_value = next_value_tensor.cpu().numpy() # CPU로 이동

        # 수집된 데이터를 numpy 배열로 변환 (메모리 효율성)
        # mb_obs는 이미 numpy list, 나머지도 numpy list
        mb_obs_np = np.asarray(mb_obs, dtype=np.float32) # (STEPS, NUM_ENVS, 4, 84, 84)
        mb_actions_np = np.asarray(mb_actions, dtype=np.int64)
        mb_logprobs_np = np.asarray(mb_logprobs, dtype=np.float32)
        mb_values_np = np.asarray(mb_values, dtype=np.float32)
        mb_rewards_np = np.asarray(mb_rewards, dtype=np.float32)
        mb_dones_np = np.asarray(mb_dones, dtype=np.bool_)

        # 데이터를 (STEPS * NUM_ENVS, ...) 형태로 펼치기
        num_steps, num_envs = mb_rewards_np.shape[0], mb_rewards_np.shape[1]
        mb_obs_flat = mb_obs_np.reshape(-1, FRAME_STACK, 84, 84)
        mb_actions_flat = mb_actions_np.reshape(-1)
        mb_logprobs_flat = mb_logprobs_np.reshape(-1)
        mb_values_flat = mb_values_np.reshape(-1)
        mb_rewards_flat = mb_rewards_np.reshape(-1)
        mb_dones_flat = mb_dones_np.reshape(-1)

        # GAE 계산 (utils.compute_gae 사용)
        returns_flat, advantages_flat = compute_gae(
            torch.from_numpy(mb_rewards_flat),
            torch.from_numpy(mb_values_flat),
            torch.from_numpy(mb_dones_flat),
            torch.from_numpy(next_value).flatten(), # next_value도 flatten 필요
            gamma=GAMMA,
            lam=LAMBDA
        )

        # 이점 정규화
        advantages_flat = (advantages_flat - advantages_flat.mean()) / (advantages_flat.std() + 1e-8)

        # PPO 업데이트 (데이터를 텐서로 변환하여 전달)
        # 주의: mb_obs_flat는 numpy 상태 유지, train_ppo 내부에서 GPU로 보냄
        train_ppo(
            model, optimizer, scaler,
            torch.from_numpy(mb_obs_flat), # CPU 상태의 텐서
            torch.from_numpy(mb_actions_flat),
            torch.from_numpy(mb_logprobs_flat),
            returns_flat, # 이미 텐서
            advantages_flat # 이미 텐서
        )

        # 학습률 스케줄러 업데이트
        scheduler.step()

    # 학습 종료 후 정리
    if not render:
        envs.close()
    print("--- 학습 종료 ---")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--render", action="store_true", help="Render the environment."
    )
    # 필요시 다른 인자 추가 (예: --tag "MyExperiment")
    # parser.add_argument("--tag", type=str, default="DefaultTag", help="Experiment tag for folder name.")
    args = parser.parse_args()

    # 태그를 인자로 받아서 폴더 이름에 사용 가능
    # if args.tag != "DefaultTag":
    #     experiment_tag = args.tag # 커맨드라인 인자로 태그 덮어쓰기

    main(args.render)

```

**`utils.py` 수정:**

여러 유틸리티 함수들이 파일 경로를 인자로 받도록 수정해야 합니다.

```python
# utils.py

import os
import re
import glob
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import imageio
import gymnasium as gym
# 필요한 다른 임포트들 (pytz, datetime, torchvision 등)

# --- 체크포인트 저장/로드 함수 수정 ---
def save_checkpoint(state, episode_num, scores, checkpoint_dir, keep_last=10):
    # checkpoint_dir 인자 사용
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    filepath = os.path.join(checkpoint_dir, f"checkpoint_{episode_num}.pth") # 에피소드 번호 기준 저장
    # state 딕셔너리에 필요한 정보 포함 확인 (model, optimizer, scaler, scheduler, update_num)
    torch.save({"state": state, "scores": scores}, filepath)
    print(f"체크포인트 저장: {filepath}")

    extract_number = lambda filename: int(re.search(r"checkpoint_(\d+).pth", os.path.basename(filename)).group(1))
    checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, "checkpoint_*.pth")), key=extract_number)

    while len(checkpoints) > keep_last:
        os.remove(checkpoints.pop(0))
        print(f"오래된 체크포인트 삭제: {checkpoints[0]}")

def load_checkpoint(checkpoint_dir):
    # checkpoint_dir 인자 사용
    if not os.path.exists(checkpoint_dir):
        print(f"체크포인트 디렉토리가 없습니다: {checkpoint_dir}")
        return None, None

    extract_number = lambda filename: int(re.search(r"checkpoint_(\d+).pth", os.path.basename(filename)).group(1))
    checkpoints = sorted(glob.glob(os.path.join(checkpoint_dir, "checkpoint_*.pth")), key=extract_number)

    if not checkpoints:
        print(f"체크포인트 파일이 없습니다 in {checkpoint_dir}")
        return None, None

    latest_checkpoint_path = checkpoints[-1]
    print(f"최신 체크포인트 로드 중: {latest_checkpoint_path}")
    try:
        checkpoint = torch.load(latest_checkpoint_path)
        # state 딕셔너리 및 scores 반환
        return checkpoint.get("state"), checkpoint.get("scores")
    except Exception as e:
        print(f"체크포인트 로드 실패: {e}")
        return None, None

# --- 그래프 그리기 함수 수정 ---
def plot_all_scores(scores, plot_prefix, csv_filename):
    """세 개의 점수 그래프를 생성하고 CSV 파일을 저장합니다."""
    if not scores:
        print("점수 데이터가 없어 그래프를 생성할 수 없습니다.")
        return

    print(f"점수 그래프 생성 중... 저장 위치 접두사: {plot_prefix}, CSV: {csv_filename}")
    # CSV 파일 저장
    try:
        df = pd.DataFrame({'score': scores})
        df.index.name = 'episode'
        df.index += 1 # 에피소드 번호 1부터 시작
        df.to_csv(csv_filename)
        print(f"점수 데이터 CSV 저장 완료: {csv_filename}")
    except Exception as e:
        print(f"CSV 저장 실패: {e}")

    # 전체 기간 그래프
    _plot_scores_internal(scores, f"{plot_prefix}scores_full.png", title="Scores per Episode (Full)", plot_moving_best=True)
    # 단순 이동 평균 그래프
    _plot_scores_internal(scores, f"{plot_prefix}scores_simple.png", title="Scores per Episode (Moving Averages)", plot_moving_best=False)
    # 최근 1000개 에피소드 그래프
    _plot_scores_internal(scores[-1000:], f"{plot_prefix}scores_recent_1000.png", title="Scores per Episode (Recent 1000)", plot_moving_best=False, offset=max(0, len(scores)-1000))

    print(f"점수 그래프 저장 완료: {plot_prefix}...")

def _plot_scores_internal(scores, filename, title="Scores per Episode", plot_moving_best=True, offset=0):
    """ 내부 그래프 그리기 함수 """
    if not scores: return
    plt.figure(figsize=(12, 6))
    episodes = np.arange(1 + offset, len(scores) + 1 + offset)

    # 이동 평균 계산 및 플롯
    if len(scores) >= 10:
        moving_avg_10 = np.convolve(scores, np.ones(10)/10, mode='valid')
        plt.plot(episodes[9:], moving_avg_10, label="10-ep MA", color="blue", alpha=0.8)
    if len(scores) >= 100:
        moving_avg_100 = np.convolve(scores, np.ones(100)/100, mode='valid')
        plt.plot(episodes[99:], moving_avg_100, label="100-ep MA", color="orange", alpha=0.8)
    if len(scores) >= 1000:
        moving_avg_1000 = np.convolve(scores, np.ones(1000)/1000, mode='valid')
        plt.plot(episodes[999:], moving_avg_1000, label="1000-ep MA", color="red", alpha=0.8)

    if plot_moving_best:
        # 최고 점수 추적
        max_scores = np.maximum.accumulate(scores)
        plt.plot(episodes, max_scores, label="Best Score", color="green", linestyle="--", alpha=0.6)
        # 100개 윈도우 이동 최고/최저
        window = 100
        if len(scores) >= window:
             moving_best = [max(scores[max(0, i - window + 1):i + 1]) for i in range(len(scores))]
             moving_worst = [min(scores[max(0, i - window + 1):i + 1]) for i in range(len(scores))]
             plt.plot(episodes, moving_best, label=f"{window}-ep Moving Best", color="purple", linestyle="-.", alpha=0.6)
             plt.plot(episodes, moving_worst, label=f"{window}-ep Moving Worst", color="brown", linestyle="-.", alpha=0.6)

    plt.xlabel("Episode")
    plt.ylabel("Score")
    plt.title(title)
    plt.legend()
    plt.grid(True, alpha=0.3)
    try:
        plt.savefig(filename)
    except Exception as e:
        print(f"그래프 저장 실패 {filename}: {e}")
    plt.close()


# --- 시각화 함수 수정 (경로 인자 추가) ---
def visualize_filters(model, layer_name, epoch, save_path):
    # save_path 아래 'filters' 폴더에 저장하도록 경로 조합
    filters_dir = os.path.join(save_path, "filters")
    os.makedirs(filters_dir, exist_ok=True)
    filepath = os.path.join(filters_dir, f"filters_{layer_name}_epoch_{epoch}.png")
    # ... (기존 시각화 로직) ...
    try:
        # weights 추출 및 시각화 코드 (기존 코드 활용)
        # plt.savefig(filepath, bbox_inches="tight")
        plt.close()
        print(f"필터 시각화 저장: {filepath}")
    except Exception as e:
        print(f"필터 시각화 저장 실패: {e}")

def visualize_layer_output(model, layer_name, save_path, epoch):
    # save_path 아래 'layers' 폴더에 저장하도록 경로 조합
    layers_dir = os.path.join(save_path, "layers")
    os.makedirs(layers_dir, exist_ok=True)
    filepath = os.path.join(layers_dir, f"output_{layer_name}_epoch_{epoch}.png")
    # ... (기존 시각화 로직) ...
    try:
        # activation 추출 및 시각화 코드 (기존 코드 활용)
        # plt.savefig(filepath, bbox_inches="tight")
        plt.close()
        print(f"레이어 출력 시각화 저장: {filepath}")
    except Exception as e:
        print(f"레이어 출력 시각화 저장 실패: {e}")


# --- 게임 플레이 GIF 저장 함수 수정 (경로 인자 추가) ---
def save_gameplay_gif(model, env_id, episode_num, max_steps, device, save_path):
    # save_path 인자 사용 (이미 gameplay_dir로 전달됨)
    os.makedirs(save_path, exist_ok=True) # gameplay 폴더 생성 확인
    filename = os.path.join(save_path, f"gameplay_episode_{episode_num}.gif")

    # 환경 생성 (렌더링 모드, 래퍼 적용 - make_env와 유사하게)
    try:
        # 여기서 env 생성 시 make_env를 직접 호출하거나 유사한 래퍼 적용
        env = gym.make(env_id, render_mode="rgb_array")
        # 주의: GIF 저장을 위한 환경에는 학습과 동일한 래퍼 적용 필요
        # (예: AtariPreprocessing, FrameStack 등), 단 보상 클리핑은 제외 가능
        env = gym.wrappers.AtariPreprocessing(env, screen_size=84, grayscale_obs=True, scale_obs=True) # scale_obs=True 확인
        env = gym.wrappers.FrameStack(env, 4)
        # TimeLimit은 여기선 불필요할 수 있음

        frames = []
        obs, _ = env.reset()
        done = False
        total_reward = 0
        current_step = 0

        model.eval() # 평가 모드로 설정
        while not done and current_step < max_steps:
            frame = env.render()
            if frame is None: # 예외 처리
                print("Warning: env.render() returned None.")
                break
            frames.append(frame)

            if isinstance(obs, tuple): obs = np.array(obs) # LazyFrames
            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)

            with torch.no_grad():
                action, _, _, _ = model.get_action_and_value(obs_tensor)
                action = action.cpu().item()

            next_obs, r, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += r
            obs = next_obs
            current_step += 1

        env.close()
        model.train() # 다시 학습 모드로

        if frames: # 프레임이 있을 때만 저장
            imageio.mimsave(filename, frames, fps=30)
            print(f"게임 플레이 GIF 저장 완료: {filename} (점수: {total_reward})")
        else:
            print(f"프레임이 없어 GIF를 저장할 수 없습니다 (에피소드 {episode_num}).")

        return total_reward

    except Exception as e:
        print(f"GIF 저장 중 오류 발생: {e}")
        if 'env' in locals() and env is not None: env.close() # 오류 시 env 닫기
        model.train() # 오류 시 학습 모드 복구
        return 0


# --- 한국 시간 반환 함수 (get_korea_time) - 수정 없음 ---
# def get_korea_time(): ...

# --- 벡터 환경 생성 함수 (make_env) - 1단계에서 수정 ---
# def make_env(...): ... (아래 1단계 내용 참고)

# --- GAE 계산 함수 (compute_gae) - 수정 없음 ---
# def compute_gae(...): ...

```

**실행:**

1.  위 코드를 `ppo_breakout.py`와 `utils.py`에 적용합니다.
2.  `ppo_breakout.py` 상단의 `experiment_tag`를 적절히 설정합니다.
3.  터미널에서 `python ppo_breakout.py`를 실행합니다.
4.  `ppo_breakout.py`가 있는 폴더의 상위 폴더에 `results` 폴더가 생성되고, 그 안에 `[타임스탬프]_[태그]` 형식의 폴더가 만들어지며 결과가 저장됩니다.

---

### 1단계: 데이터 정규화 및 환경 표준화 (가장 중요)

**목표:** `make_env` 함수를 수정하여 `AtariPreprocessing`과 `TransformReward` 래퍼를 사용합니다.

**`utils.py`의 `make_env` 함수 수정:**

```python
# utils.py

# 상단에 임포트 추가
from gymnasium.wrappers import AtariPreprocessing, TransformReward, FrameStack, TimeLimit
import numpy as np

def make_env(env_id, render=False, max_episode_steps=27000, frame_stack=4, clip_rewards=True):
    """
    환경을 생성하는 함수 (AtariPreprocessing 사용 최적화)

    Args:
        env_id (str): 환경 ID
        render (bool): 렌더링 모드 여부
        max_episode_steps (int): 최대 스텝 수 (TimeLimit)
        frame_stack (int): 프레임 스택 수
        clip_rewards (bool): 보상을 -1, 0, 1로 클리핑할지 여부

    Returns:
        Callable: 환경 생성 함수
    """
    render_mode = "rgb_array" if render else None # GIF 저장을 위해 render=True시 rgb_array 사용

    def _thunk():
        env = gym.make(env_id, render_mode=render_mode)

        # Atari 전처리 래퍼 적용
        # - noop_max=30: 시작 시 랜덤 No-Op (초기 상태 다양화)
        # - frame_skip=4: 프레임 스킵 (연산 효율성, 일반적 설정)
        # - screen_size=84: 관측 크기 조정
        # - terminal_on_life_loss=True: 목숨 잃으면 done=True 처리 (매우 중요)
        # - grayscale_obs=True: 흑백 변환
        # - scale_obs=True: 관측값을 [0, 1] 범위 float로 자동 스케일링
        env = AtariPreprocessing(env, noop_max=30, frame_skip=4, screen_size=84, terminal_on_life_loss=True, grayscale_obs=True, scale_obs=True)

        # 프레임 스택 래퍼 적용
        env = FrameStack(env, frame_stack)

        # 보상 클리핑 래퍼 (-1, 0, 1) 적용 (학습 안정화)
        if clip_rewards:
            env = TransformReward(env, lambda r: np.sign(r))

        # 타임 리밋 래퍼 (에피소드 최대 길이 제한)
        if max_episode_steps is not None:
            env = TimeLimit(env, max_episode_steps=max_episode_steps)

        return env
    return _thunk

# --- 나머지 utils.py 함수들 ---
```

**`ppo_breakout.py` 수정:**

*   `make_env` 호출 부분을 확인하여 `frame_stack=FRAME_STACK`과 `clip_rewards=True` (학습 시) 인자가 잘 전달되는지 확인합니다. (위 0단계 코드에 이미 반영됨)
*   `PPONet`의 `forward` 함수에서 수동으로 `/ 255.0` 하는 코드가 있다면 **반드시 제거**합니다. (위 0단계 코드에 이미 반영됨)

---

### 2단계: 핵심 하이퍼파라미터 조정 (롤아웃 길이 및 학습률)

**목표:** `STEPS_PER_UPDATE`를 늘리고, `LEARNING_RATE`를 낮추며 선형 감쇠 스케줄러를 적용합니다.

**`ppo_breakout.py` 상단 하이퍼파라미터 섹션 수정:**

```python
# ppo_breakout.py 상단

# ... (다른 하이퍼파라미터) ...
LEARNING_RATE = 0.00025   # 2.5e-4 (Atari PPO 표준값 중 하나)
# OPTIMIZER_GAMMA = 0.999 # ExponentialLR 사용 안 함
STEPS_PER_UPDATE = 1024   # 롤아웃 길이 증가 (예: 128 -> 1024 or 2048)
# 주의: STEPS_PER_UPDATE * NUM_ENVS 만큼의 메모리가 필요합니다. (1024 * 32 = 32768 샘플)
# ... (다른 하이퍼파라미터) ...
```

**`ppo_breakout.py`의 `main` 함수 내 Optimizer 및 Scheduler 초기화 부분 수정:**

```python
# ppo_breakout.py main 함수 내

    # ... (모델 생성 후) ...

    print(f"초기 학습률: {LEARNING_RATE}")
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, eps=1e-5) # Adam epsilon 추가 (안정성)

    # --- 학습률 선형 감쇠 스케줄러 설정 ---
    # 총 업데이트 횟수 정의 (메인 루프 반복 횟수)
    num_updates = 100_001 # 또는 더 길게 (예: 500_000, 1_000_000)

    # LambdaLR을 사용하여 선형 감쇠 구현
    # update_num은 1부터 num_updates까지 증가한다고 가정
    lr_lambda = lambda update_num: max(0.0, 1.0 - (update_num / float(num_updates + 1)))
    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)

    scaler = GradScaler(enabled=torch.cuda.is_available()) # CUDA 사용 가능할 때만 활성화

    # ... (체크포인트 로드 부분) ...
    # 체크포인트 로드 시 scheduler 상태도 로드 확인
    if checkpoint_data and "scheduler_state" in checkpoint_data:
        scheduler.load_state_dict(checkpoint_data["scheduler_state"])
        print(f"스케줄러 상태 로드 완료. Last LR: {scheduler.get_last_lr()[0]:.2e}")

    # ... (메인 학습 루프 시작 전) ...

    # 메인 학습 루프 (num_updates 사용)
    print(f"총 업데이트 수행 예정: {num_updates}")
    start_update_num = start_episode # 체크포인트에서 로드한 업데이트 번호
    if start_update_num > 0:
        # 로드된 스케줄러 상태에 맞춰 현재 스텝 반영 (중요!)
        # LambdaLR은 step() 호출 횟수 기준이므로, 로드된 상태가 현재 스텝을 반영함
        print(f"학습 재개: 업데이트 {start_update_num+1}부터 시작.")
        # 필요시 scheduler.last_epoch = start_update_num 설정 가능성 검토 (LambdaLR 동작 방식 확인)

    for update in range(start_update_num + 1, num_updates + 1):
        # ... (데이터 수집 로직) ...

        # --- GAE 계산 및 PPO 업데이트 ---
        # ... (GAE 계산) ...
        # ... (이점 정규화) ...

        # PPO 업데이트 호출
        train_ppo(model, optimizer, scaler, ...)

        # --- 학습률 스케줄러 업데이트 ---
        # 중요: LambdaLR은 optimizer.step() 후에 scheduler.step()을 호출해야 함
        # 여기서는 매 PPO 업데이트(여러번의 optimizer.step() 포함) 후에 한 번 호출
        scheduler.step() # 현재 update 번호를 기반으로 LR 계산 및 적용

        # ... (로그 출력, 저장 등) ...
        # 로그 출력 시 현재 LR 확인: scheduler.get_last_lr()[0]
```

---

### 3단계: PPO 업데이트 방식 최적화 (미니배치 및 에포크)

**목표:** `train_ppo` 함수를 수정하여 명시적인 `MINI_BATCH_SIZE`와 `PPO_EPOCHS`를 사용합니다.

**`ppo_breakout.py` 상단 하이퍼파라미터 섹션 수정:**

```python
# ppo_breakout.py 상단

# ... (다른 하이퍼파라미터) ...
NUM_ENVS = 32
STEPS_PER_UPDATE = 1024  # 2단계에서 설정됨
PPO_EPOCHS = 4           # 에포크 수 (4 ~ 10 사이 시도, 4부터 시작)
MINI_BATCH_SIZE = 256    # 미니배치 크기 (예: 256 or 512)
                         # NUM_ENVS * STEPS_PER_UPDATE (32768) 가 MINI_BATCH_SIZE (256) 로 나누어 떨어지면 좋음 (32768 / 256 = 128 미니배치)
# ... (다른 하이퍼파라미터) ...
```

**`ppo_breakout.py`의 `train_ppo` 함수 전체 수정:**

```python
# ppo_breakout.py

def train_ppo(model, optimizer, scaler, all_obs, all_actions, all_old_log_probs, all_returns, all_advantages):
    """ PPO 업데이트 함수 (미니배치 및 에포크 사용) """
    # 입력 데이터는 모두 CPU 텐서라고 가정 (GPU 메모리 절약 위함)
    batch_size = all_obs.size(0) # 전체 데이터 크기 (예: 32768)
    num_minibatches = batch_size // MINI_BATCH_SIZE # 미니배치 개수 (예: 128)
    if num_minibatches == 0: # 데이터가 미니배치 크기보다 작으면 전체 사용
        num_minibatches = 1
        effective_mini_batch_size = batch_size
    else:
        effective_mini_batch_size = MINI_BATCH_SIZE

    # print(f"PPO Update: Batch={batch_size}, MiniBatchSize={effective_mini_batch_size}, NumMiniBatches={num_minibatches}, Epochs={PPO_EPOCHS}")

    for _ in range(PPO_EPOCHS):
        # 매 에포크 시작 시 전체 데이터 인덱스 셔플
        indices = torch.randperm(batch_size)

        for i in range(num_minibatches):
            # 미니배치 인덱스 계산
            start = i * effective_mini_batch_size
            end = (i + 1) * effective_mini_batch_size
            mini_batch_indices = indices[start:end]

            # 미니배치 데이터 슬라이싱 (CPU -> GPU 전송)
            # 필요한 데이터만 GPU로 보냄
            mb_obs = all_obs[mini_batch_indices].to(device)
            mb_actions = all_actions[mini_batch_indices].to(device)
            mb_old_log_probs = all_old_log_probs[mini_batch_indices].to(device)
            mb_returns = all_returns[mini_batch_indices].to(device)
            mb_advantages = all_advantages[mini_batch_indices].to(device)

            # --- PPO 손실 계산 및 업데이트 (기존 로직과 거의 동일) ---
            with autocast(enabled=scaler.is_enabled()):
                # 현재 정책으로 값 계산
                new_actions, new_log_probs, entropy, values = model.get_action_and_value(mb_obs, mb_actions) # 액션도 전달해야 해당 액션의 로그확률 계산

                # PPO 비율 계산
                log_ratio = new_log_probs - mb_old_log_probs
                ratio = torch.exp(log_ratio)

                # 이점 정규화 (미니배치 단위, 선택 사항 - 보통 전체 배치 단위로 함)
                # mb_adv = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
                mb_adv = mb_advantages # 전체 배치에서 이미 정규화됨

                # 정책 손실 (Surrogate Loss)
                surr1 = ratio * mb_adv
                surr2 = torch.clamp(ratio, 1.0 - PPO_EPSILON, 1.0 + PPO_EPSILON) * mb_adv
                actor_loss = -torch.min(surr1, surr2).mean()

                # 가치 손실 (Value Loss) - MSE
                # 필요시 가치 클리핑 추가 가능 (선택적)
                critic_loss = F.mse_loss(values, mb_returns)

                # 엔트로피 손실 (탐험 장려)
                entropy_loss = -entropy.mean()

                # 전체 손실
                # ENTROPY_COEF는 4단계에서 동적 조절 가능
                loss = actor_loss + VALUE_COEF * critic_loss + ENTROPY_COEF * entropy_loss

            # 그래디언트 계산 및 업데이트
            optimizer.zero_grad()
            scaler.scale(loss).backward() # 스케일된 손실로 backward
            scaler.unscale_(optimizer) # 스케일 복원 (클리핑 전)

            # 그래디언트 클리핑 (복원된 그래디언트에 적용)
            nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)

            # 옵티마이저 스텝 (스케일러 사용)
            scaler.step(optimizer)
            scaler.update() # 스케일러 상태 업데이트

# --- 나머지 ppo_breakout.py 코드 ---
```

---

### 4단계: 장기 보상 및 탐험 조정 (선택적 미세 조정)

**목표:** `GAMMA` 값을 약간 높이고, `ENTROPY_COEF`를 학습 진행에 따라 감소시킵니다.

**`ppo_breakout.py` 상단 하이퍼파라미터 섹션 수정:**

```python
# ppo_breakout.py 상단

# ... (다른 하이퍼파라미터) ...
GAMMA = 0.995 # 또는 0.99 (기본값), 0.999 등 시도
ENTROPY_COEF_INITIAL = 0.01 # 초기 엔트로피 계수
ENTROPY_COEF_FINAL = 0.001 # 최종 엔트로피 계수 (예시)
# ... (다른 하이퍼파라미터) ...
```

**`ppo_breakout.py`의 `train_ppo` 함수 내 손실 계산 부분 수정:**

```python
# ppo_breakout.py train_ppo 함수 내, 미니배치 루프 안

            # --- PPO 손실 계산 및 업데이트 ---
            with autocast(enabled=scaler.is_enabled()):
                # ... (actor_loss, critic_loss 계산) ...

                # 엔트로피 손실
                entropy_loss = -entropy.mean()

                # --- 엔트로피 계수 선형 감쇠 적용 ---
                # 현재 업데이트 비율 계산 (update 번호 필요 -> train_ppo 인자로 전달하거나 전역 변수 사용 필요)
                # 여기서는 간단히 update 변수가 접근 가능하다고 가정 (실제로는 main 루프의 update 변수 전달 필요)
                # current_update_fraction = update / num_updates # main의 update와 num_updates 사용
                # current_entropy_coef = ENTROPY_COEF_INITIAL * (1 - current_update_fraction) + ENTROPY_COEF_FINAL * current_update_fraction
                # current_entropy_coef = max(current_entropy_coef, ENTROPY_COEF_FINAL) # 최소값 보장

                # 임시: main의 update 변수를 train_ppo에 전달했다고 가정
                # train_ppo(..., current_update_fraction):
                # current_entropy_coef = np.interp(current_update_fraction, [0, 1], [ENTROPY_COEF_INITIAL, ENTROPY_COEF_FINAL])

                # 우선은 고정값 사용 (감쇠 구현은 추가 작업 필요)
                current_entropy_coef = ENTROPY_COEF_INITIAL

                # 전체 손실 계산
                loss = actor_loss + VALUE_COEF * critic_loss + current_entropy_coef * entropy_loss

            # ... (그래디언트 계산 및 업데이트) ...

```

**참고:** 엔트로피 계수 감쇠를 구현하려면 `main` 함수의 `update` 변수를 `train_ppo` 함수에 전달하거나, `train_ppo`가 현재 진행률을 알 수 있는 다른 방법이 필요합니다. 우선은 고정값(`ENTROPY_COEF_INITIAL`)으로 시작하고, 성능 개선이 더 필요할 때 감쇠를 구현하는 것이 좋습니다.

---

**최종 확인:**

*   모든 파일 경로가 새로운 `output_dir` 및 하위 폴더(`checkpoint_dir` 등)를 사용하도록 수정되었는지 확인합니다.
*   `utils.py`의 함수들이 경로 인자를 제대로 받아서 처리하는지 확인합니다.
*   하이퍼파라미터 값들이 의도대로 변경되었는지 확인합니다.
*   `make_env` 함수가 `AtariPreprocessing`을 올바르게 사용하는지 확인합니다.
*   `train_ppo` 함수가 새로운 미니배치 로직으로 수정되었는지 확인합니다.
*   학습률 스케줄러가 `LambdaLR` (선형 감쇠)로 설정되었는지 확인합니다.

이제 수정된 코드를 실행하여 성능 변화를 관찰합니다. 필요에 따라 하이퍼파라미터(특히 `LEARNING_RATE`, `STEPS_PER_UPDATE`, `MINI_BATCH_SIZE`, `PPO_EPOCHS`)를 추가적으로 조정해볼 수 있습니다.

20231027_1100_PPO_Breakout_Baseline (변경 전 초기 실행)

20231027_1430_PPO_Breakout_AtariWrap_ClipRwd (1단계 적용 후)

20231028_0915_PPO_Breakout_LR2e-4_Rollout1024 (2단계 적용 후)

20250412_0847_PPO_Breakout_ExpDecayMinLR (2-1단계 적용 후)

20231028_1800_PPO_Breakout_MiniBatch256_Epoch10 (3단계 적용 후)

20231029_1000_PPO_Breakout_Gamma995_EntDecay (4단계 적용 후)